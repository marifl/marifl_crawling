# ğŸ•·ï¸ marifl_crawling - Smart Website Crawler

[![Python Version](https://img.shields.io/badge/python-3.10%2B-blue)](https://www.python.org/downloads/)
[![License](https://img.shields.io/badge/license-MIT-green)](LICENSE.md)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)

Ein leistungsstarker, intelligenter Website-Crawler, der speziell fÃ¼r das Scrapen von **Bookdown-Seiten** entwickelt wurde. Er behÃ¤lt die richtige Kapitelreihenfolge aus der Website-Navigation bei und eignet sich perfekt fÃ¼r das Herunterladen von Online-BÃ¼chern und Dokumentationen. Entwickelt mit [`crawl4ai`](https://github.com/unclecode/crawl4ai) fÃ¼r robuste Crawling-Funktionen.

> ğŸ¤– Dieses Projekt wurde in Zusammenarbeit mit GitHub Copilot entwickelt

## âœ¨ Funktionen

- ğŸ§  **Intelligente Navigationsanalyse**: Extrahiert und bewahrt automatisch die richtige Kapitelreihenfolge aus der Website-Navigation
- ğŸ“‘ **Mehrere Ausgabeformate**: Du kannst nach HTML oder Markdown exportieren
- ğŸ”¢ **Intelligente Dateinummerierung**: Dynamische Pufferung basierend auf der Gesamtanzahl der Seiten
- ğŸ”— **Link-Erhaltung**: Aktualisiert automatisch interne Links, um mit den neuen Dateinamen Ã¼bereinzustimmen
- ğŸš€ **Erweiterte Optionen**: Wiederholungslogik, Ratenbegrenzung, Filterung und mehr
- ğŸ“Š **Fortschrittsverfolgung**: Echtzeit-Fortschrittsupdates und detaillierte Protokollierung
- ğŸ—‚ï¸ **Indexerstellung**: Erstellt JSON-Index und Markdown-Ãœbersicht des gecrawlten Inhalts

## ğŸ“‹ Voraussetzungen

- Python 3.10 oder hÃ¶her
- AbhÃ¤ngigkeiten werden Ã¼ber `pyproject.toml` verwaltet

## ğŸš€ Installation

### Mit uv (Empfohlen)

```bash
# Repository klonen
git clone https://github.com/marifl/marifl_crawling.git
cd marifl_crawling

# Mit uv installieren
uv sync

# Setze crawl4ai ein (zwingend erforderlich fÃ¼r Browser-Setup)
uv run crawl4ai-setup

# Optional: Installiere als uv tool fÃ¼r direkten Zugriff
uv tool install .

# Jetzt kannst du 'scrwl' direkt verwenden ohne 'uv run'
# Das Tool ist nun GLOBAL verfÃ¼gbar - du kannst es von Ã¼berall auf deinem Computer ausfÃ¼hren!
# Weitere Informationen: https://docs.astral.sh/uv/guides/tools/
```

### Mit pip

```bash
# Repository klonen
git clone https://github.com/marifl/marifl_crawling.git
cd marifl_crawling

# Im Entwicklungsmodus installieren
pip install -e .

# Setup crawl4ai (zwingend erforderlich fÃ¼r Browser-Setup)
python -m crawl4ai.install
```

## ğŸ“– Verwendung

**Wichtig**: Es gibt zwei Wege, den `scrwl` Befehl zu verwenden:

1. **Mit `uv tool install .`** (empfohlen): 
   - `scrwl` wird **global** installiert und ist von **Ã¼berall** auf deinem Computer verfÃ¼gbar
   - Du kannst `scrwl` direkt aus jedem Verzeichnis ausfÃ¼hren
   - Solange `uv` installiert bleibt, funktioniert `scrwl` system-weit
   
2. **Ohne tool install**: 
   - Du musst `uv run scrwl` verwenden
   - Nur innerhalb des Repository-Verzeichnisses verfÃ¼gbar

### Grundlegende Verwendung

```bash
# Mit uv tool install (direkter Zugriff)
scrwl -u https://example.com/docs/index.html -o ./output -f html

# Ohne tool install (Ã¼ber uv run)
uv run scrwl -u https://example.com/docs/index.html -o ./output -f html

# Crawlen und als Markdown speichern
scrwl -u https://example.com/docs/index.html -o ./output -f md
```

### ğŸŒ Globale VerfÃ¼gbarkeit

Nach der Installation mit `uv tool install .` kannst du `scrwl` von **Ã¼berall** auf deinem Computer verwenden:

```bash
# Von deinem Home-Verzeichnis
cd ~
scrwl -u https://r4ds.had.co.nz/index.html -o ./downloads/r4ds -f md

# Von jedem beliebigen Verzeichnis
cd /tmp
scrwl -u https://bookdown.org/yihui/rmarkdown/index.html -o ./books/rmarkdown -f md

# Solange uv installiert ist, funktioniert scrwl system-weit!
```

**Vorteile der globalen Installation:**
- âœ… Funktioniert aus jedem Verzeichnis
- âœ… Keine Notwendigkeit, ins Repository-Verzeichnis zu wechseln
- âœ… KÃ¼rzere Befehle (kein `uv run` erforderlich)
- âœ… Integration in Scripts und Automatisierung

> ğŸ“– **Weitere Informationen Ã¼ber uv tools**: [UV Tools Documentation](https://docs.astral.sh/uv/guides/tools/)

### Erweiterte Verwendung

```bash
# Mit VerzÃ¶gerung zwischen den Anfragen (empfohlen fÃ¼r groÃŸe Seiten)
scrwl -u https://example.com/docs -o ./output -f md -d 0.5

# Vorhandene Dateien Ã¼berspringen (nÃ¼tzlich zum Fortsetzen unterbrochener Crawls)
scrwl -u https://example.com/docs -o ./output -f md -s

# Trockenlauf, um eine Vorschau darauf zu erhalten, was gecrawlt werden wÃ¼rde
scrwl -u https://example.com/docs -o ./output -f md -n

# Ausgabe bereinigen (entfernt Skripte, Stile, Metatags)
scrwl -u https://example.com/docs -o ./output -f md -c

# Navigationselemente aus der Ausgabe entfernen
scrwl -u https://example.com/docs -o ./output -f md -N

# Seiten nach Regex-Muster filtern
scrwl -u https://example.com/docs -o ./output -f md -fil "chapter-[1-3]|introduction"

# Begrenzung auf die ersten 10 Seiten
scrwl -u https://example.com/docs -o ./output -f md -m 10

# Kombination mehrerer Optionen
scrwl -u https://example.com/docs -o ./output -f md -d 1.0 -c -N -m 20
```

### ğŸ“š Bookdown-spezifische Nutzung

Dieses Tool wurde speziell fÃ¼r das Scrapen von Bookdown-Seiten entwickelt. Bookdown erstellt Online-BÃ¼cher mit einer strukturierten Kapitelnavigation, die der Crawler intelligent erkennt und bewahrt.

```bash
# Typische Bookdown-Seite crawlen
scrwl -u https://bookdown.org/yihui/rmarkdown/index.html -o ./rmarkdown_book -f md -d 0.5 -c

# R fÃ¼r Data Science Buch (beliebtes Bookdown-Beispiel)
scrwl -u https://r4ds.had.co.nz/index.html -o ./r4ds -f md -d 1.0 -c -N

# Akademisches Buch mit spezifischen Kapiteln
scrwl -u https://example.bookdown.org/book/index.html -o ./academic_book -f md -fil "chapter-[1-5]|introduction|conclusion" -c
```

**Bookdown-spezifische Funktionen:**
- âœ… Erkennt automatisch Bookdown-Navigation
- âœ… Bewahrt Kapitelreihenfolge bei
- âœ… Korrigiert interne Links zwischen Kapiteln
- âœ… Entfernt Bookdown-spezifische UI-Elemente bei Bedarf
- âœ… Optimiert fÃ¼r typische Bookdown-Strukturen

### Weitere Anwendungsbeispiele

```bash
# Dokumentation mit bestimmten Kapiteln nur crawlen
scrwl -u https://docs.python.org/3/tutorial/index.html -o ./python_tutorial -f md -fil "introduction|data-structures|classes" -c

# Einen unterbrochenen Crawl fortsetzen
scrwl -u https://large-docs.example.com -o ./large_docs -f html -s -d 1.0
```

## ğŸ› ï¸ Befehlszeilenoptionen

### Erforderliche Argumente

| Option | Beschreibung |
|--------|-------------|
| `-u, --url` | Start-URL der zu crawlenden Website |
| `-o, --output` | Ausgabeverzeichnis fÃ¼r gespeicherte Dateien |
| `-f, --format` | Ausgabeformat: `html` oder `md` |

### Optionale Argumente

| Option | Beschreibung | Standard |
|--------|-------------|---------|
| `-d, --delay` | VerzÃ¶gerung in Sekunden zwischen den Anfragen | 0 |
| `-t, --timeout` | Timeout in Sekunden fÃ¼r jede Anfrage | 30 |
| `-r, --max-retries` | Maximale Anzahl von Wiederholungsversuchen fÃ¼r fehlgeschlagene Anfragen | 3 |
| `-s, --skip-existing` | Dateien Ã¼berspringen, die bereits existieren | False |
| `-n, --dry-run` | Vorschau darauf, was gecrawlt werden wÃ¼rde, ohne zu speichern | False |
| `-N, --no-navigation` | Navigationselemente aus der Ausgabe entfernen | False |
| `-c, --clean` | Skripte, Stile und Metatags entfernen | False |
| `-fil, --filter` | Regex-Muster zum Filtern von Seiten nach Titel oder URL | None |
| `-m, --max-pages` | Maximale Anzahl von Seiten, die gecrawlt werden sollen | None |
| `-v, --verbose` | Detaillierte Debug-Informationen anzeigen | False |

## ğŸ“ Ausgabestruktur

Der Crawler erstellt die folgende Struktur in deinem Ausgabeverzeichnis:

```
output_directory/
â”œâ”€â”€ index.json          # JSON-Index aller gecrawlten Seiten
â”œâ”€â”€ README.md           # Ãœbersicht fÃ¼r Menschen
â”œâ”€â”€ 00_Introduction.md  # Nummerierte Inhaltsdateien
â”œâ”€â”€ 01_Chapter_One.md
â”œâ”€â”€ 02_Chapter_Two.md
â””â”€â”€ ...
```

### Dateibenennungskonvention

- Dateien sind mit dynamischer Pufferung nummeriert (z. B. `01_`, `001_`, `0001_`)
- Kapitelnumerierungen werden aus der Quelle beibehalten
- Unnummerierte Seiten erhalten eine Null-Pufferung (z. B. `00_`, `000_`)
- Sonderzeichen werden aus Dateinamen entfernt
- Dateinamen werden auf 40 Zeichen gekÃ¼rzt

### Indexdateien

**index.json**: Maschinenlesbarer Index
```json
{
  "format": "md",
  "total_chapters": 15,
  "files": [
    {
      "filename": "01_Introduction.md",
      "url": "https://example.com/intro.html",
      "chapter_number": "1",
      "title": "Introduction",
      "index": 0
    }
  ]
}
```

**README.md**: Ãœbersicht fÃ¼r Menschen mit einer Tabelle aller gecrawlten Seiten

## ğŸ† Best Practices

### 1. **Verwende immer VerzÃ¶gerungen fÃ¼r groÃŸe Seiten**
```bash
scrwl -u https://example.com -o ./output -f md -d 1.0
```
Dies verhindert, dass der Server Ã¼berlastet wird, und verringert die Wahrscheinlichkeit, dass du eine Ratenbegrenzung erhÃ¤ltst.

### 2. **Beginne mit einem Trockenlauf**
```bash
scrwl -u https://example.com -o ./output -f md -n
```
Vorschau darauf, was gecrawlt wird, bevor du dich fÃ¼r einen vollstÃ¤ndigen Crawl entscheidest.

### 3. **Verwende die bereinigte Ausgabe zum Lesen**
```bash
scrwl -u https://example.com -o ./output -f md -c -N
```
Entfernt unnÃ¶tige Elemente fÃ¼r ein saubereres Leseerlebnis.

### 4. **Setze unterbrochene Crawls fort**
```bash
scrwl -u https://example.com -o ./output -f md -s
```
Die `-s`-Option Ã¼berspringt vorhandene Dateien, perfekt zum Fortsetzen.

### 5. **Filtere nach bestimmten Inhalten**
```bash
scrwl -u https://docs.example.com -o ./output -f md -fil "api|reference"
```
Crawle nur Seiten, die deinem Muster entsprechen.

### 6. **Setze angemessene Timeouts**
```bash
scrwl -u https://slow-site.com -o ./output -f md -t 60
```
ErhÃ¶he das Timeout fÃ¼r langsame Seiten.

## ğŸ” So funktioniert's

1. **Navigationsanalyse**: Der Crawler analysiert zuerst die Startseite, um alle Navigationslinks zu extrahieren
2. **Reihenfolgenbestimmung**: Er erkennt intelligent die Kapitelnumerierungen und sortiert die Seiten entsprechend
3. **Intelligentes Crawlen**: Seiten werden in der richtigen Reihenfolge mit konfigurierbaren VerzÃ¶gerungen gecrawlt
4. **Linkkorrektur**: Interne Links werden automatisch aktualisiert, um mit den neuen Dateinamen Ã¼bereinzustimmen
5. **Indexerstellung**: Erstellt sowohl JSON- als auch Markdown-Indexes fÃ¼r eine einfache Navigation

## ğŸ› Fehlerbehebung

### HÃ¤ufige Probleme

**Ratenbegrenzung**
- LÃ¶sung: ErhÃ¶he die VerzÃ¶gerung mit `-d 2.0` oder hÃ¶her

**Timeout-Fehler**
- LÃ¶sung: ErhÃ¶he das Timeout mit `-t 60`

**Speicherprobleme bei groÃŸen Seiten**
- LÃ¶sung: Verwende `-m 100`, um die Seiten pro Durchlauf zu begrenzen

**Kodierungsfehler**
- Der Crawler verarbeitet standardmÃ¤ÃŸig UTF-8
- Sonderzeichen werden im Inhalt beibehalten

### Debug-Modus

Verwende `-v` fÃ¼r ausfÃ¼hrliche Ausgaben zur Diagnose von Problemen:
```bash
scrwl -u https://example.com -o ./output -f md -v
```

## ğŸ¤ Mitwirken

BeitrÃ¤ge sind willkommen! Bitte zÃ¶gere nicht, einen Pull Request einzureichen. Bei grÃ¶ÃŸeren Ã„nderungen Ã¶ffne bitte zuerst ein Issue, um zu besprechen, was du Ã¤ndern mÃ¶chtest.

### Entwicklungseinrichtung

```bash
# Repo klonen
git clone https://github.com/marifl/marifl_crawling.git
cd marifl_crawling

# Im Entwicklungsmodus installieren
uv sync

# Tests ausfÃ¼hren (wenn verfÃ¼gbar)
pytest
```

## ğŸ“„ Lizenz

Dieses Projekt ist lizenziert unter der MIT-Lizenz - siehe die [LICENSE.md](LICENSE.md)-Datei fÃ¼r Details.

## ğŸ™ Danksagungen

- Basierend auf [crawl4ai](https://github.com/unclecode/crawl4ai) fÃ¼r robustes Web-Crawling
- Verwendet [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/) zur HTML-Analyse
- [html2text](https://github.com/Alir3z4/html2text/) zur Markdown-Konvertierung

## ğŸ“® UnterstÃ¼tzung

Wenn du auf Probleme stÃ¶ÃŸt oder Fragen hast:

1. ÃœberprÃ¼fe den Abschnitt [Fehlerbehebung](#-fehlerbehebung)
2. Suche nach [bestehenden Problemen](https://github.com/marifl/marifl_crawling/issues)
3. Erstelle ein neues Issue mit:
   - Deinem Befehl
   - Fehlermeldung
   - Python-Version
   - Betriebssystem

---

Hergestellt mit â¤ï¸ von marifl in Zusammenarbeit mit GitHub Copilot

---

Mancherorts kann crawling nicht erlaubt sein. PrÃ¼fe das vorher. Jeder ist fÃ¼r sich selbst verantwortlich, also Ã¼bernimmm die Verantwortung fÃ¼r dein eigenes Handeln! - alles ist "as is" kein gewÃ¤hr fÃ¼r irgendwas, Nutzung auf eigene Gefahr usw.....