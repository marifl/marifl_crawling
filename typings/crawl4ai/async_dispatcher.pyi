"""
This type stub file was generated by pyright.
"""

import asyncio
from typing import List, Optional, Tuple
from .async_configs import CrawlerRunConfig
from .models import CrawlerTaskResult
from .components.crawler_monitor import CrawlerMonitor
from .types import AsyncWebCrawler
from collections.abc import AsyncGenerator
from abc import ABC, abstractmethod

class RateLimiter:
    def __init__(self, base_delay: Tuple[float, float] = ..., max_delay: float = ..., max_retries: int = ..., rate_limit_codes: List[int] = ...) -> None:
        ...
    
    def get_domain(self, url: str) -> str:
        ...
    
    async def wait_if_needed(self, url: str) -> None:
        ...
    
    def update_delay(self, url: str, status_code: int) -> bool:
        ...
    


class BaseDispatcher(ABC):
    def __init__(self, rate_limiter: Optional[RateLimiter] = ..., monitor: Optional[CrawlerMonitor] = ...) -> None:
        ...
    
    @abstractmethod
    async def crawl_url(self, url: str, config: CrawlerRunConfig, task_id: str, monitor: Optional[CrawlerMonitor] = ...) -> CrawlerTaskResult:
        ...
    
    @abstractmethod
    async def run_urls(self, urls: List[str], crawler: AsyncWebCrawler, config: CrawlerRunConfig, monitor: Optional[CrawlerMonitor] = ...) -> List[CrawlerTaskResult]:
        ...
    


class MemoryAdaptiveDispatcher(BaseDispatcher):
    def __init__(self, memory_threshold_percent: float = ..., critical_threshold_percent: float = ..., recovery_threshold_percent: float = ..., check_interval: float = ..., max_session_permit: int = ..., fairness_timeout: float = ..., rate_limiter: Optional[RateLimiter] = ..., monitor: Optional[CrawlerMonitor] = ...) -> None:
        ...
    
    async def crawl_url(self, url: str, config: CrawlerRunConfig, task_id: str, retry_count: int = ...) -> CrawlerTaskResult:
        ...
    
    async def run_urls(self, urls: List[str], crawler: AsyncWebCrawler, config: CrawlerRunConfig) -> List[CrawlerTaskResult]:
        ...
    
    async def run_urls_stream(self, urls: List[str], crawler: AsyncWebCrawler, config: CrawlerRunConfig) -> AsyncGenerator[CrawlerTaskResult, None]:
        ...
    


class SemaphoreDispatcher(BaseDispatcher):
    def __init__(self, semaphore_count: int = ..., max_session_permit: int = ..., rate_limiter: Optional[RateLimiter] = ..., monitor: Optional[CrawlerMonitor] = ...) -> None:
        ...
    
    async def crawl_url(self, url: str, config: CrawlerRunConfig, task_id: str, semaphore: asyncio.Semaphore = ...) -> CrawlerTaskResult:
        ...
    
    async def run_urls(self, crawler: AsyncWebCrawler, urls: List[str], config: CrawlerRunConfig) -> List[CrawlerTaskResult]:
        ...
    


