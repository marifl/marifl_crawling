"""
This type stub file was generated by pyright.
"""

from abc import ABC, abstractmethod
from typing import Any, Dict, List, Optional
from bs4 import PageElement, Tag
from lxml import etree, html as lhtml
from .models import ScrapingResult

OG_REGEX = ...
TWITTER_REGEX = ...
DIMENSION_REGEX = ...
def parse_srcset(s: str) -> List[Dict]:
    ...

def parse_dimension(dimension): # -> tuple[int, str | Any] | tuple[None, None]:
    ...

def fetch_image_file_size(img, base_url): # -> str | None:
    ...

class ContentScrapingStrategy(ABC):
    @abstractmethod
    def scrap(self, url: str, html: str, **kwargs) -> ScrapingResult:
        ...
    
    @abstractmethod
    async def ascrap(self, url: str, html: str, **kwargs) -> ScrapingResult:
        ...
    


class WebScrapingStrategy(ContentScrapingStrategy):
    """
    Class for web content scraping. Perhaps the most important class.

    How it works:
    1. Extract content from HTML using BeautifulSoup.
    2. Clean the extracted content using a content cleaning strategy.
    3. Filter the cleaned content using a content filtering strategy.
    4. Generate markdown content from the filtered content.
    5. Return the markdown content.
    """
    def __init__(self, logger=...) -> None:
        ...
    
    def scrap(self, url: str, html: str, **kwargs) -> ScrapingResult:
        """
        Main entry point for content scraping.

        Args:
            url (str): The URL of the page to scrape.
            html (str): The HTML content of the page.
            **kwargs: Additional keyword arguments.

        Returns:
            ScrapingResult: A structured result containing the scraped content.
        """
        ...
    
    async def ascrap(self, url: str, html: str, **kwargs) -> ScrapingResult:
        """
        Main entry point for asynchronous content scraping.

        Args:
            url (str): The URL of the page to scrape.
            html (str): The HTML content of the page.
            **kwargs: Additional keyword arguments.

        Returns:
            ScrapingResult: A structured result containing the scraped content.
        """
        ...
    
    def is_data_table(self, table: Tag, **kwargs) -> bool:
        """
        Determine if a table element is a data table (not a layout table).

        Args:
            table (Tag): BeautifulSoup Tag representing a table element
            **kwargs: Additional keyword arguments including table_score_threshold

        Returns:
            bool: True if the table is a data table, False otherwise
        """
        ...
    
    def extract_table_data(self, table: Tag) -> dict:
        """
        Extract structured data from a table element.
        
        Args:
            table (Tag): BeautifulSoup Tag representing a table element
            
        Returns:
            dict: Dictionary containing table data (headers, rows, caption, summary)
        """
        ...
    
    def flatten_nested_elements(self, node): # -> NavigableString:
        """
        Flatten nested elements in a HTML tree.

        Args:
            node (Tag): The root node of the HTML tree.

        Returns:
            Tag: The flattened HTML tree.
        """
        ...
    
    def find_closest_parent_with_useful_text(self, tag, **kwargs): # -> None:
        """
        Find the closest parent with useful text.

        Args:
            tag (Tag): The starting tag to search from.
            **kwargs: Additional keyword arguments.

        Returns:
            Tag: The closest parent with useful text, or None if not found.
        """
        ...
    
    def remove_unwanted_attributes(self, element, important_attrs, keep_data_attributes=...): # -> None:
        """
        Remove unwanted attributes from an HTML element.

        Args:
            element (Tag): The HTML element to remove attributes from.
            important_attrs (list): List of important attributes to keep.
            keep_data_attributes (bool): Whether to keep data attributes.

        Returns:
            None
        """
        ...
    
    def process_image(self, img, url, index, total_images, **kwargs):
        """
        Process an image element.

        How it works:
        1. Check if the image has valid display and inside undesired html elements.
        2. Score an image for it's usefulness.
        3. Extract image file metadata to extract size and extension.
        4. Generate a dictionary with the processed image information.
        5. Return the processed image information.

        Args:
            img (Tag): The image element to process.
            url (str): The URL of the page containing the image.
            index (int): The index of the image in the list of images.
            total_images (int): The total number of images in the list.
            **kwargs: Additional keyword arguments.

        Returns:
            dict: A dictionary containing the processed image information.
        """
        ...
    
    def process_element(self, url, element: PageElement, **kwargs) -> Dict[str, Any]:
        """
        Process an HTML element.

        How it works:
        1. Check if the element is an image, video, or audio.
        2. Extract the element's attributes and content.
        3. Process the element based on its type.
        4. Return the processed element information.

        Args:
            url (str): The URL of the page containing the element.
            element (Tag): The HTML element to process.
            **kwargs: Additional keyword arguments.

        Returns:
            dict: A dictionary containing the processed element information.
        """
        ...
    


class LXMLWebScrapingStrategy(WebScrapingStrategy):
    def __init__(self, logger=...) -> None:
        ...
    
    def find_closest_parent_with_useful_text(self, element: lhtml.HtmlElement, **kwargs) -> Optional[str]:
        ...
    
    def flatten_nested_elements(self, element: lhtml.HtmlElement) -> lhtml.HtmlElement:
        """Flatten nested elements of the same type in LXML tree"""
        ...
    
    def process_image(self, img: lhtml.HtmlElement, url: str, index: int, total_images: int, **kwargs) -> Optional[List[Dict]]:
        ...
    
    def remove_empty_elements_fast(self, root, word_count_threshold=...):
        """
        Remove elements that fall below the desired word threshold in a single pass from the bottom up.
        Skips non-element nodes like HtmlComment and bypasses certain tags that are allowed to have no content.
        """
        ...
    
    def remove_unwanted_attributes_fast(self, root: lhtml.HtmlElement, important_attrs=..., keep_data_attributes=...) -> lhtml.HtmlElement:
        """
        Removes all attributes from each element (including root) except those in `important_attrs`.
        If `keep_data_attributes=True`, also retain any attribute starting with 'data-'.

        Returns the same root element, mutated in-place, for fluent usage.
        """
        ...
    
    def is_data_table(self, table: etree.Element, **kwargs) -> bool:
        ...
    
    def extract_table_data(self, table: etree.Element) -> dict:
        ...
    


